=============
Mixture Guide
=============

A Mixture object is a linear combination of Component objects that describes a complex distribution. The Mixture object state describes the distribution in terms of the Component
objects (which have their own paramters) and a list of weights that describe the relative
importance of each Component. Using the `Expectation-Maximization algorithm <https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm>`_, a Mixture object can fit itself to an input data array of shape ``(n_samples, n_features)`` where a "sample" is a single star (or can be treated as such, e.g. a combined binary) and a feature is any data we're trying to fit to (position, velocity, magnitude, apparent-magnitude, abundances, inferred-age, etc.). The Mixture object will remain completely agnostic of the features being fit. The implementation of how features are fit to is left to the :class:`~chronostar.base.BaseComponent` class.

See :ref:`Fitting a Mixture <scripts-mixture>` for an example script and
:ref:`Fitting a mixture <cli_mix>` for instructions on how to use the command line
tool ``fit-mixture``.


Implemented Mixtures 
--------------------

.. _guide-compmix:

:class:`~chronostar.mixture.componentmixture.ComponentMixture`
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Chronostar's sole implemented Mixture Model matches the interface of `scikit-learn's mixture models <https://scikit-learn.org/stable/modules/mixture.html#gmm>`_ as closely as possible, inheriting from `sklearn.mixture.BaseMixture <https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator>`_. For reference of a class that inherits from BaseMixture, see `sklearn.mixture.GaussianMixture <https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture>`_.


The key method of the ComponentMixture is its :func:`~chronostar.mixture.componentmixture.ComponentMixture.fit` method.
In this instance, it calls :func:`~chronostar.mixture.sklmixture.SKLComponentMixture.fit` which is a `scikit-learn
method <https://github.com/scikit-learn/scikit-learn/blob/02ebf9e68fe1fc7687d9e1047b9e465ae0fd945e/sklearn/mixture/_base.py#L159>`_.
This performs the Expectation-Maximization algorithm which we explain briefly here.

The Expectation-Maximization (EM) algorithm consists of two steps which are
executed iteratively until convergence is reached. Underlying this algorithm is
the assumption that each data point as a true membership to one of the components,
or equivalently, for each data point, there is one component that is responsible
for it. The E-step's purpose is to estimate these membership probabilities as a value between 0 and 1. This is done by, for each data point, calculating the
probability that it was generated by each component's distribution (i.e. evaluate
each Component's PDF at the position of the data point) and then normalizing these
probabilities such that they sum to 1. The E-step is implemented by 
`scikit-learn's base Class <https://github.com/scikit-learn/scikit-learn/blob/02ebf9e68fe1fc7687d9e1047b9e465ae0fd945e/sklearn/mixture/_base.py#L293>`_
which calls the abstract method ``_estimate_log_prob`` which is defined
in :class:`~chronostar.mixture.sklmixture.SKLComponentMixture`.

Eqiupped with membership probabilites, the M-step can maximize the parameters
of each Component in isolation. Since each Component has its own defined
:func:`~chronostar.base.BaseComponent.maximize` method, the M-step implementation
loops over the mixture's components, maximizing them one by one.

