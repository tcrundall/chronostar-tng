#!/usr/bin/env python
import argparse
from astropy.table import Table
import numpy as np
from pathlib import Path

from chronostar import datatools
from chronostar.utils import coordinate, transform


def convert_astro2cart(astr_mean, astr_cov):
    """
    Convert astrometry data (mean and covariance) into cartesian
    coordinates, centred on the local standard of rest (Schoenrich 2010).

    Parameters
    ----------
    astr_mean: [6] float array_like
        The central estimate of a star's astrometry values. Provided in
        the order:
            ra [deg]
            dec [deg]
            parallax [mas]
            pmra*cos(dec) [mas/yr]
            pmdec [mas/yr]
            radial velocity [km/s]
    astr_cov: [6,6] float array_like
        The covariance matrix of the measurments with columns (and rows)
        in same order as `astr_mean`.

    Returns
    -------
    xyzuvw_mean: [6] float array_like
        The cartesian mean (XYZUVW)
    xyzuvw_cov: [6,6] float array_like
        The carteisan covariance matrix
    """
    xyzuvw_cov, xyzuvw_mean = transform.transform_covmatrix_py(
            cov=astr_cov, trans_func=coordinate.convert_astrometry2lsrxyzuvw,
            loc=astr_mean
    )

    return xyzuvw_mean, xyzuvw_cov


def init_argparse() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(
        description="Prepare data files for input into Chronostar"
    )
    # parser.add_argument("-c", "--config", help="The configuration file")
    parser.add_argument(
        "astrodata",
        help="The astro data file, stored as a .fits file"
    )

    parser.add_argument(
        "-n",
        "--nomerge",
        help="Whether to skip merging means and covariances into one data array",
        action="store_true",
    )
    parser.add_argument(
        "-f",
        "--fakervs",
        help="Replace missing RVs with fake ones",
        action="store_true",
    )
    parser.add_argument(
        "-o",
        "--output",
        help="Output directory",
        default=".",
    )

    return parser


if __name__ == '__main__':

    parser = init_argparse()
    args = parser.parse_args()

    # Set up save directory (lest we simply crash at the end)
    savedir = Path(args.output)

    if not savedir.is_dir():
        savedir.mkdir(parents=True)

    # Read in astropy table
    t = Table.read(args.astrodata)

    COLUMN_STEMS = [
        'ra', 'dec', 'parallax', 'pmra', 'pmdec', 'radial_velocity'
    ]

    # Substitute fake radial velocities for any that are missing
    fake_rv = np.mean(t['radial_velocity'])
    FAKE_RV_ERROR = 10_000.  # km/s
    if args.fakervs:
        missing_mask = ~ np.isfinite(t['radial_velocity'])
        np.save(savedir / "fake_rvs_ids.npy", t['source_id'][np.where(missing_mask)])
        t['radial_velocity'][missing_mask] = fake_rv
        t['radial_velocity_error'][missing_mask] = FAKE_RV_ERROR

    # Define mask for all valid rows
    msk = np.isfinite(t['radial_velocity'])
    for column in COLUMN_STEMS:
        msk &= np.isfinite(t[column])

    import ipdb; ipdb.set_trace()

    # Extract astrometry data, 6 measurements, 6 errors, 15 correlations
    # Assume gaia formatting
    astrodata = np.zeros((len(msk), 6 + 6 + 15))

    dim = len(COLUMN_STEMS)
    # Insert measurements
    for i, stem in enumerate(COLUMN_STEMS):
        astrodata[:, i] = t[stem][msk]

    # Insert errors
    error_col_offset = dim
    for i, stem in enumerate(COLUMN_STEMS):
        error_col_name = f"{stem}_error"
        astrodata[:, error_col_offset + i] = t[error_col_name][msk]

    # Insert correlations
    corr_col = 2 * dim
    for i in range(len(COLUMN_STEMS)):
        for j in range(i+1, len(COLUMN_STEMS)):
            corr_col_name = f"{COLUMN_STEMS[i]}_{COLUMN_STEMS[j]}_corr"
            if corr_col_name in t.keys():
                astrodata[:, corr_col] = t[corr_col_name][msk]
            corr_col += 1

    # Insert covariances
    corr_col = 2 * dim
    for i in range(len(COLUMN_STEMS)):
        for j in range(i+1, len(COLUMN_STEMS)):
            corr_col_name = f"{COLUMN_STEMS[i]}_{COLUMN_STEMS[j]}_corr"
            error_1 = f"{COLUMN_STEMS[i]}_error"
            error_2 = f"{COLUMN_STEMS[j]}_error"
            if corr_col_name in t.keys():
                astrodata[:, corr_col] =\
                    t[corr_col_name][msk] * t[error_1][msk] * t[error_2][msk]
            corr_col += 1

    # Convert astrometry data to cartesian (with covs?)
    astro_means, astro_covs = datatools.construct_covs_from_data(astrodata)

    assert not np.any(np.isnan(astro_means))
    assert not np.any(np.isnan(astro_covs))

    cart_means = np.empty(astro_means.shape)
    cart_covs = np.empty(astro_covs.shape)

    print("Converting to cartesian coordinates")
    for i in range(len(msk)):
        if i % 100 == 0:
            print(f"Converted {i/len(msk)*100:5.1f}% of input", end='\r')
        cart_means[i], cart_covs[i] = convert_astro2cart(
            astro_means[i], astro_covs[i],
        )
    print(f"Converted {100:5.1f}% of input", end='\r')
    print("\nDone!")

    # Store data in numpy arrays
    np.save(savedir / "data_means.npy", cart_means)
    np.save(savedir / "data_covs.npy", cart_covs)

    # Store ids
    np.save(savedir / "ids.npy", t['source_id'][msk])

    # Merge into one data array if requested
    if not args.nomerge:
        cart_all = np.vstack((cart_means.T, cart_covs.flatten().reshape(-1, 36).T)).T
        np.save(savedir / "data_all.npy", cart_all)
